{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e706c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "데이터 로드 시작\n",
      "Train shape: (10704179, 119)\n",
      "Test shape: (1527298, 119)\n",
      "데이터 로드 완료\n",
      "Num features: 113 | Cat features: 4\n",
      "gender unique categories: 3\n",
      "age_group unique categories: 9\n",
      "inventory_id unique categories: 18\n",
      "l_feat_14 unique categories: 3286\n",
      "모델 학습 실행\n",
      "학습 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 1]: 100%|██████████| 10454/10454 [4:14:56<00:00,  1.46s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 1.2100\n",
      "[DEBUG] GPU Allocated: 26.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 2]: 100%|██████████| 10454/10454 [6:37:45<00:00,  2.28s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 1.1880\n",
      "[DEBUG] GPU Allocated: 26.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 3]: 100%|██████████| 10454/10454 [6:51:57<00:00,  2.36s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 1.1771\n",
      "[DEBUG] GPU Allocated: 27.31 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 4]: 100%|██████████| 10454/10454 [6:50:06<00:00,  2.35s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 1.1814\n",
      "[DEBUG] GPU Allocated: 27.03 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 5]: 100%|██████████| 10454/10454 [6:48:57<00:00,  2.35s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 1.1723\n",
      "[DEBUG] GPU Allocated: 27.22 MB\n",
      "학습 완료\n",
      "추론 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Inference]: 100%|██████████| 1492/1492 [09:56<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 완료\n",
      "제출 파일 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "CFG = {\n",
    "    'BATCH_SIZE': 1024,\n",
    "    'EPOCHS': 5,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'SEED': 42\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"데이터 로드 시작\")\n",
    "train = pd.read_parquet(\"./train.parquet\", engine=\"pyarrow\")\n",
    "test = pd.read_parquet(\"./test.parquet\", engine=\"pyarrow\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(\"데이터 로드 완료\")\n",
    "\n",
    "target_col = \"clicked\"\n",
    "seq_col = \"seq\"\n",
    "FEATURE_EXCLUDE = {target_col, seq_col, \"ID\"}\n",
    "feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]\n",
    "\n",
    "cat_cols = [\"gender\", \"age_group\", \"inventory_id\", \"l_feat_14\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "print(f\"Num features: {len(num_cols)} | Cat features: {len(cat_cols)}\")\n",
    "\n",
    "def encode_categoricals(train_df, test_df, cat_cols):\n",
    "    encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        all_values = pd.concat([train_df[col], test_df[col]], axis=0).astype(str).fillna(\"UNK\")\n",
    "        le.fit(all_values)\n",
    "        train_df[col] = le.transform(train_df[col].astype(str).fillna(\"UNK\"))\n",
    "        test_df[col]  = le.transform(test_df[col].astype(str).fillna(\"UNK\"))\n",
    "        encoders[col] = le\n",
    "        print(f\"{col} unique categories: {len(le.classes_)}\")\n",
    "    return train_df, test_df, encoders\n",
    "\n",
    "train, test, cat_encoders = encode_categoricals(train, test, cat_cols)\n",
    "\n",
    "class ClickDataset(Dataset):\n",
    "    def __init__(self, df, num_cols, cat_cols, seq_col, target_col=None, has_target=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.seq_col = seq_col\n",
    "        self.target_col = target_col\n",
    "        self.has_target = has_target\n",
    "        self.num_X = self.df[self.num_cols].astype(float).fillna(0).values\n",
    "        self.cat_X = self.df[self.cat_cols].astype(int).values\n",
    "        self.seq_strings = self.df[self.seq_col].astype(str).values\n",
    "        if self.has_target:\n",
    "            self.y = self.df[self.target_col].astype(np.float32).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        num_x = torch.tensor(self.num_X[idx], dtype=torch.float)\n",
    "        cat_x = torch.tensor(self.cat_X[idx], dtype=torch.long)\n",
    "        s = self.seq_strings[idx]\n",
    "        if s:\n",
    "            arr = np.fromstring(s, sep=\",\", dtype=np.float32)\n",
    "        else:\n",
    "            arr = np.array([0.0], dtype=np.float32)\n",
    "        seq = torch.from_numpy(arr)\n",
    "        if self.has_target:\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.float)\n",
    "            return num_x, cat_x, seq, y\n",
    "        else:\n",
    "            return num_x, cat_x, seq\n",
    "\n",
    "def collate_fn_train(batch):\n",
    "    num_x, cat_x, seqs, ys = zip(*batch)\n",
    "    num_x = torch.stack(num_x)\n",
    "    cat_x = torch.stack(cat_x)\n",
    "    ys = torch.stack(ys)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
    "    return num_x, cat_x, seqs_padded, seq_lengths, ys\n",
    "\n",
    "def collate_fn_infer(batch):\n",
    "    num_x, cat_x, seqs = zip(*batch)\n",
    "    num_x = torch.stack(num_x)\n",
    "    cat_x = torch.stack(cat_x)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
    "    return num_x, cat_x, seqs_padded, seq_lengths\n",
    "\n",
    "class CrossNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_dim, 1, bias=True) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x0):\n",
    "        x = x0\n",
    "        for w in self.layers:\n",
    "            x = x0 * w(x) + x\n",
    "        return x\n",
    "\n",
    "class WideDeepCTR(nn.Module):\n",
    "    def __init__(self, num_features, cat_cardinalities, emb_dim=16, lstm_hidden=64,\n",
    "                 hidden_units=[512,256,128], dropout=[0.1,0.2,0.3]):\n",
    "        super().__init__()\n",
    "        self.emb_layers = nn.ModuleList([\n",
    "            nn.Embedding(cardinality, emb_dim) for cardinality in cat_cardinalities\n",
    "        ])\n",
    "        cat_input_dim = emb_dim * len(cat_cardinalities)\n",
    "        self.bn_num = nn.BatchNorm1d(num_features)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden,\n",
    "                            num_layers=2, batch_first=True, bidirectional=True)\n",
    "        seq_out_dim = lstm_hidden * 2\n",
    "        self.cross = CrossNetwork(num_features + cat_input_dim + seq_out_dim, num_layers=2)\n",
    "        input_dim = num_features + cat_input_dim + seq_out_dim\n",
    "        layers = []\n",
    "        for i, h in enumerate(hidden_units):\n",
    "            layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout[i % len(dropout)])]\n",
    "            input_dim = h\n",
    "        layers += [nn.Linear(input_dim, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, num_x, cat_x, seqs, seq_lengths):\n",
    "        num_x = self.bn_num(num_x)\n",
    "        cat_embs = [emb(cat_x[:, i]) for i, emb in enumerate(self.emb_layers)]\n",
    "        cat_feat = torch.cat(cat_embs, dim=1)\n",
    "        seqs = seqs.unsqueeze(-1)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(seqs, seq_lengths.cpu(),\n",
    "                                                   batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        z = torch.cat([num_x, cat_feat, h], dim=1)\n",
    "        z_cross = self.cross(z)\n",
    "        out = self.mlp(z_cross)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "def train_model(train_df, num_cols, cat_cols, seq_col, target_col, batch_size, epochs, lr, device):\n",
    "    train_dataset = ClickDataset(train_df, num_cols, cat_cols, seq_col, target_col, True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=collate_fn_train, pin_memory=True)\n",
    "    cat_cardinalities = [len(cat_encoders[c].classes_) for c in cat_cols]\n",
    "    model = WideDeepCTR(\n",
    "        num_features=len(num_cols),\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        emb_dim=16,\n",
    "        lstm_hidden=64,\n",
    "        hidden_units=[512,256,128],\n",
    "        dropout=[0.1,0.2,0.3]\n",
    "    ).to(device)\n",
    "    pos_weight_value = (len(train_df) - train_df[target_col].sum()) / train_df[target_col].sum()\n",
    "    pos_weight = torch.tensor([pos_weight_value], dtype=torch.float).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2)\n",
    "    print(\"학습 시작\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for num_x, cat_x, seqs, lens, ys in tqdm(train_loader, desc=f\"[Train Epoch {epoch}]\"):\n",
    "            num_x, cat_x, seqs, lens, ys = num_x.to(device), cat_x.to(device), seqs.to(device), lens.to(device), ys.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(num_x, cat_x, seqs, lens)\n",
    "            loss = criterion(logits, ys)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item() * ys.size(0)\n",
    "        total_loss /= len(train_dataset)\n",
    "        print(f\"[Epoch {epoch}] Train Loss: {total_loss:.4f}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"[DEBUG] GPU Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "    print(\"학습 완료\")\n",
    "    return model\n",
    "\n",
    "print(\"모델 학습 실행\")\n",
    "model = train_model(\n",
    "    train_df=train,\n",
    "    num_cols=num_cols,\n",
    "    cat_cols=cat_cols,\n",
    "    seq_col=seq_col,\n",
    "    target_col=target_col,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    epochs=CFG['EPOCHS'],\n",
    "    lr=CFG['LEARNING_RATE'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"추론 시작\")\n",
    "test_dataset = ClickDataset(test, num_cols, cat_cols, seq_col, has_target=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False,\n",
    "                         collate_fn=collate_fn_infer, pin_memory=True)\n",
    "model.eval()\n",
    "outs = []\n",
    "with torch.no_grad():\n",
    "    for num_x, cat_x, seqs, lens in tqdm(test_loader, desc=\"[Inference]\"):\n",
    "        num_x, cat_x, seqs, lens = num_x.to(device), cat_x.to(device), seqs.to(device), lens.to(device)\n",
    "        outs.append(torch.sigmoid(model(num_x, cat_x, seqs, lens)).cpu())\n",
    "test_preds = torch.cat(outs).numpy()\n",
    "print(\"추론 완료\")\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['clicked'] = test_preds\n",
    "submit.to_csv('./test.csv', index=False)\n",
    "print(\"제출 파일 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytroch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
