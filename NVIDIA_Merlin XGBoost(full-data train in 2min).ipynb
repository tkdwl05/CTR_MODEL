{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Í≤ΩÏßÑÎåÄÌöåÏóêÏÑú Í∞ÄÏ†∏Ïò® ÏΩîÎìú\n",
    "\n",
    "\n",
    "ÏïàÎÖïÌïòÏÑ∏Ïöî,\n",
    "Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏùÄ Ïù¥Ï†ÑÎ∂ÄÌÑ∞ Í¥ÄÏã¨ ÏûàÎäî Î∂ÑÏïºÏòÄÎäîÎç∞\n",
    "Ïã§Ï†ú Í∏∞ÏóÖ Îç∞Ïù¥ÌÑ∞Î•º Îã§Î£∞ Ïàò ÏûàÎäî Ï¢ãÏùÄ ÎåÄÌöå Ïó¥Ïñ¥ Ï£ºÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§.\n",
    "\n",
    "Î≥∏ ÎåÄÌöåÏùò Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ ÏïΩ 1000Îßå ÌñâÏù¥Í∏∞ ÎïåÎ¨∏Ïóê\n",
    "Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏùΩÎäî ÏàúÍ∞Ñ RAM overflowÍ∞Ä ÏùºÏñ¥ÎÇòÍ∏∞ ÏâΩÏäµÎãàÎã§.\n",
    "polarsÎ•º ÏÇ¨Ïö©ÌïòÎäî Î∞©Î≤ïÎèÑ ÏûàÏúºÎÇò, Ïù¥ ÏΩîÎìúÏóêÏÑúÎäî ÌäπÌûà CTR Í≥ºÏ†úÏóê ÌäπÌôîÎêú ÌîÑÎ†àÏûÑÏõåÌÅ¨Ïù∏ NVIDIA MerlinÏùÑ ÏÜåÍ∞úÌï©ÎãàÎã§.\n",
    "\n",
    "ÏΩîÎìúÎäî XGBoost Í∏∞Ï§ÄÏúºÎ°ú ÏûëÏÑ±ÌñàÏúºÎÇò normalization Ï†ÅÏö© Ïãú NN Í≥ÑÏó¥ Î™®Îç∏ÏóêÎèÑ Ï†ÅÏö© Í∞ÄÎä•Ìï©ÎãàÎã§.\n",
    "Îçî ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©Ïù¥ Í∂ÅÍ∏àÌïòÏãúÎã§Î©¥ Í≥µÏãù ÌôàÌéòÏù¥ÏßÄÏôÄ ÍπÉÌóàÎ∏å ÏΩîÎìúÎ•º Ï∞∏Í≥†Ìï¥ Î≥¥ÏãúÎ©¥ Ï¢ãÏäµÎãàÎã§.\n",
    "(https://developer.nvidia.com/merlin)\n",
    "\n",
    "RTX A6000 Í∏∞Ï§ÄÏúºÎ°ú Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ 5fold-CV(stratifiedKfold) ÌïôÏäµÏóê ÏïΩ 2Î∂Ñ Í∞ÄÎüâÏù¥ ÏÜåÏöîÎê©ÎãàÎã§.\n",
    "ÏÜåÏöî ÏãúÍ∞ÑÏù¥ ÏßßÏùÄ Ïù¥Ïú†Îäî ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§.\n",
    "\n",
    "1. GPU-Accelerated Data Processing\n",
    "- cuDFÎäî GPU DataFrameÏúºÎ°ú pandasÎ≥¥Îã§ 10~50Î∞∞ Îπ†Î¶ÖÎãàÎã§.\n",
    "- RAPIDS ÌîÑÎ†àÏûÑÏõåÌÅ¨Ïóê ÌÜµÌï©ÎêòÏñ¥ cuML, cuGraph Î∞è CUDA ÏµúÏ†ÅÌôîÍ∞Ä ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "2. Out-of-Core Processing\n",
    "- ÎåÄÏö©Îüâ Îç∞Ïù¥ÌÑ∞(CTR ÌäπÏú†Ïùò high-cardinalityÏó¥ ÎåÄÏùë), Ïä§ÎßàÌä∏ Ï∫êÏã±, Î©îÎ™®Î¶¨ Ïä§ÌïÑÎßÅ(GPU->CPU->ÎîîÏä§ÌÅ¨) Îì±ÏùÑ ÏßÄÏõêÌï©ÎãàÎã§.\n",
    "\n",
    "3. Columnar Memory Layout\n",
    "- Merlin ÌîÑÎ†àÏûÑÏõåÌÅ¨ ÏûêÏ≤¥Í∞Ä .parquetÏóê ÏµúÏ†ÅÌôîÎêòÏñ¥ cuDF+parquet Ï°∞Ìï©ÏúºÎ°ú ÏÜçÎèÑÎ•º ÎçîÏö± Í∞ÄÏÜçÌï©ÎãàÎã§.\n",
    "- Ïª¨ÎüºÎ≥Ñ ÏïïÏ∂ï/Ïù∏ÏΩîÎî© Î∞è Î≤°ÌÑ∞Ìôî Ïó∞ÏÇ∞ÏùÑ ÏßÄÏõêÌï©ÎãàÎã§.\n",
    "\n",
    "ÏΩîÎìúÏóêÎäî Í∞ÑÎã®Ìïú class weightÏù¥ Ï†ÅÏö©ÎêòÏñ¥ ÏûàÍ≥†, seq Ïó¥ÏùÄ ÌïôÏäµÏóêÏÑú Ï†úÏô∏ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n",
    "Í∞êÏÇ¨Ìï©ÎãàÎã§.\n",
    "\n",
    "[baseline score ÎπÑÍµê]\n",
    "CV Score: 0.353183 ¬± 0.000688\n",
    "LB Score: 0.3308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Merlin XGBoost\n",
    "Complete implementation with proper memory management and debugging outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nvtabular       23.08.00        (required: ‚â•23.08.00)\n",
      "‚úÖ cudf            23.10.02        (required: ‚â•23.10)\n",
      "‚úÖ cupy            13.6.0          (required: ‚â•13.6)\n",
      "‚úÖ xgboost         3.0.5           (required: ‚â•3.0)\n",
      "‚úÖ dask            2023.9.2        (required: ‚â•2023.9)\n",
      "‚úÖ pandas          1.5.3           (required: ‚â•1.5)\n",
      "‚úÖ numpy           1.24.3          (required: ‚â•1.24)\n",
      "‚úÖ scikit-learn    1.7.1           (required: ‚â•1.7)\n",
      "‚úÖ psutil          5.9.1           (required: ‚â•5.9)\n",
      "‚úÖ pyarrow         12.0.1          (required: ‚â•12.0)\n",
      "\n",
      "‚úÖ All required libraries are installed and compatible!\n"
     ]
    }
   ],
   "source": [
    "# Required libraries and versions\n",
    "required_libs = {\n",
    "    'nvtabular': '23.08.00',\n",
    "    'cudf': '23.10',      # Prefix match\n",
    "    'cupy': '13.6',       # Prefix match  \n",
    "    'xgboost': '3.0',     # Minimum version\n",
    "    'dask': '2023.9',\n",
    "    'pandas': '1.5',\n",
    "    'numpy': '1.24',\n",
    "    'scikit-learn': '1.7',\n",
    "    'psutil': '5.9',      # 5.9.1 works fine (used in working code)\n",
    "    'pyarrow': '12.0'     # 12.0.1 works fine (used in working code)\n",
    "}\n",
    "\n",
    "# Check installed versions\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    try:\n",
    "        import pkg_resources\n",
    "    except:\n",
    "        pkg_resources = None\n",
    "\n",
    "missing_libs = []\n",
    "all_good = True\n",
    "\n",
    "for lib, required_version in required_libs.items():\n",
    "    try:\n",
    "        # Map library names for import\n",
    "        import_name = lib\n",
    "        if lib == 'scikit-learn':\n",
    "            import_name = 'sklearn'\n",
    "        \n",
    "        # Check if library is installed\n",
    "        module = importlib.import_module(import_name)\n",
    "        \n",
    "        # Get installed version\n",
    "        try:\n",
    "            if hasattr(module, '__version__'):\n",
    "                installed_version = module.__version__\n",
    "            elif pkg_resources:\n",
    "                installed_version = pkg_resources.get_distribution(lib).version\n",
    "            else:\n",
    "                installed_version = 'unknown'\n",
    "        except:\n",
    "            installed_version = 'unknown'\n",
    "        \n",
    "        # Check version compatibility\n",
    "        req_major = required_version.split('.')[0]\n",
    "        inst_version_parts = installed_version.split('.')\n",
    "        inst_major = inst_version_parts[0] if installed_version != 'unknown' else ''\n",
    "        \n",
    "        # More lenient version check\n",
    "        if installed_version == 'unknown':\n",
    "            print(f\"‚ö†Ô∏è  {lib:15} {installed_version:15} (required: ‚â•{required_version})\")\n",
    "        elif float(inst_major) >= float(req_major) if inst_major.isdigit() and req_major.isdigit() else installed_version.startswith(required_version[:3]):\n",
    "            print(f\"‚úÖ {lib:15} {installed_version:15} (required: ‚â•{required_version})\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {lib:15} {installed_version:15} (required: ‚â•{required_version}) - but should work\")\n",
    "        \n",
    "    except ImportError:\n",
    "        missing_libs.append(lib)\n",
    "        print(f\"‚ùå {lib:15} NOT INSTALLED (required: ‚â•{required_version})\")\n",
    "        all_good = False\n",
    "\n",
    "# Report\n",
    "if missing_libs:\n",
    "    print(f\"\\n‚ùå Missing libraries: {', '.join(missing_libs)}\")\n",
    "    print(\"Please install them using conda or pip\")\n",
    "elif all_good:\n",
    "    print(\"\\n‚úÖ All required libraries are installed and compatible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n",
      "NVTabular version: 23.08.00\n",
      "XGBoost version: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import psutil\n",
    "\n",
    "# GPU libraries\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular import ops\n",
    "from merlin.io import Dataset\n",
    "\n",
    "# ML libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"NVTabular version: {nvt.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration:\n",
      "   Input: /RSNA/toss/data/train.parquet\n",
      "   Output: /RSNA/toss/data/nvt_processed_final\n",
      "   Folds: 5\n",
      "   Force reprocess: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration(DATA PATH)\n",
    "TRAIN_PATH = '/RSNA/toss/data/train.parquet'\n",
    "OUTPUT_DIR = '/RSNA/toss/data/nvt_processed_final'\n",
    "TEMP_DIR = '/tmp'\n",
    "N_FOLDS = 5\n",
    "FORCE_REPROCESS = False  # Set to True to reprocess data\n",
    "\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"   Input: {TRAIN_PATH}\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "print(f\"   Folds: {N_FOLDS}\")\n",
    "print(f\"   Force reprocess: {FORCE_REPROCESS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory functions:\n",
      "üíæ CPU: 58.5GB/472.2GB (13.1%)\n",
      "üíæ GPU: 1.2GB/48.0GB\n",
      "üßπ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Memory management functions\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    \n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_used = gpu_info.used / 1024**3\n",
    "        gpu_total = gpu_info.total / 1024**3\n",
    "    except:\n",
    "        gpu_used = 0\n",
    "        gpu_total = 0\n",
    "    \n",
    "    print(f\"üíæ CPU: {mem.used/1024**3:.1f}GB/{mem.total/1024**3:.1f}GB ({mem.percent:.1f}%)\")\n",
    "    print(f\"üíæ GPU: {gpu_used:.1f}GB/{gpu_total:.1f}GB\")\n",
    "    return mem.percent\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "    print(\"üßπ GPU memory cleared\")\n",
    "\n",
    "# Test memory functions\n",
    "print(\"Testing memory functions:\")\n",
    "print_memory()\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metric functions defined\n"
     ]
    }
   ],
   "source": [
    "# Metric functions\n",
    "def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Calculate Weighted LogLoss with 50:50 class weights\"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    mask_0 = (y_true == 0)\n",
    "    mask_1 = (y_true == 1)\n",
    "    \n",
    "    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0\n",
    "    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0\n",
    "    \n",
    "    return 0.5 * ll_0 + 0.5 * ll_1\n",
    "\n",
    "def calculate_competition_score(y_true, y_pred):\n",
    "    \"\"\"Calculate competition score: 0.5*AP + 0.5*(1/(1+WLL))\"\"\"\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    wll = calculate_weighted_logloss(y_true, y_pred)\n",
    "    score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n",
    "    return score, ap, wll\n",
    "\n",
    "print(\"‚úÖ Metric functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Creating XGBoost-optimized workflow...\n",
      "   Categorical: 5 columns\n",
      "   Continuous: 112 columns\n",
      "   Total features: 117\n",
      "   ‚úÖ Workflow created (no normalization for tree models)\n",
      "‚úÖ Workflow creation tested successfully\n"
     ]
    }
   ],
   "source": [
    "def create_workflow():\n",
    "    \"\"\"Create NVTabular workflow optimized for XGBoost\"\"\"\n",
    "    print(\"\\nüîß Creating XGBoost-optimized workflow...\")\n",
    "    \n",
    "    # TRUE CATEGORICAL COLUMNS (only 5)\n",
    "    true_categorical = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    \n",
    "    # CONTINUOUS COLUMNS (112 total)\n",
    "    all_continuous = (\n",
    "        [f'feat_a_{i}' for i in range(1, 19)] +  # 18\n",
    "        [f'feat_b_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_c_{i}' for i in range(1, 9)] +   # 8\n",
    "        [f'feat_d_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_e_{i}' for i in range(1, 11)] +  # 10\n",
    "        [f'history_a_{i}' for i in range(1, 8)] +  # 7\n",
    "        [f'history_b_{i}' for i in range(1, 31)] + # 30\n",
    "        [f'l_feat_{i}' for i in range(1, 28)]      # 27\n",
    "    )\n",
    "    \n",
    "    print(f\"   Categorical: {len(true_categorical)} columns\")\n",
    "    print(f\"   Continuous: {len(all_continuous)} columns\")\n",
    "    print(f\"   Total features: {len(true_categorical) + len(all_continuous)}\")\n",
    "    \n",
    "    # Minimal preprocessing for XGBoost\n",
    "    cat_features = true_categorical >> ops.Categorify(\n",
    "        freq_threshold=0,\n",
    "        max_size=50000\n",
    "    )\n",
    "    cont_features = all_continuous >> ops.FillMissing(fill_val=0)\n",
    "    \n",
    "    workflow = nvt.Workflow(cat_features + cont_features + ['clicked'])\n",
    "    \n",
    "    print(\"   ‚úÖ Workflow created (no normalization for tree models)\")\n",
    "    return workflow\n",
    "\n",
    "# Test workflow creation\n",
    "test_workflow = create_workflow()\n",
    "print(\"‚úÖ Workflow creation tested successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ NVTabular Data Processing\n",
      "======================================================================\n",
      "‚úÖ Using existing processed data from /RSNA/toss/data/nvt_processed_final\n"
     ]
    }
   ],
   "source": [
    "def process_data():\n",
    "    \"\"\"Process data with NVTabular\"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ NVTabular Data Processing\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if already processed\n",
    "    if os.path.exists(OUTPUT_DIR) and not FORCE_REPROCESS:\n",
    "        try:\n",
    "            test_dataset = Dataset(OUTPUT_DIR, engine='parquet')\n",
    "            print(f\"‚úÖ Using existing processed data from {OUTPUT_DIR}\")\n",
    "            return OUTPUT_DIR\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Existing data corrupted, reprocessing...\")\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    # Clear existing if needed\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        print(f\"üóëÔ∏è Removing existing directory {OUTPUT_DIR}\")\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    initial_mem = print_memory()\n",
    "    \n",
    "    # Prepare data without 'seq' column\n",
    "    temp_path = f'{TEMP_DIR}/train_no_seq.parquet'\n",
    "    if not os.path.exists(temp_path):\n",
    "        print(\"\\nüìã Creating temp file without 'seq' column...\")\n",
    "        pf = pq.ParquetFile(TRAIN_PATH)\n",
    "        cols = [c for c in pf.schema.names if c != 'seq']\n",
    "        print(f\"   Total columns: {len(pf.schema.names)}\")\n",
    "        print(f\"   Using columns: {len(cols)} (excluded 'seq')\")\n",
    "        \n",
    "        df = pd.read_parquet(TRAIN_PATH, columns=cols)\n",
    "        print(f\"   Loaded {len(df):,} rows\")\n",
    "        df.to_parquet(temp_path, index=False)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        print(\"   ‚úÖ Temp file created\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Using existing temp file: {temp_path}\")\n",
    "    \n",
    "    # Create dataset with small partitions\n",
    "    print(\"\\nüì¶ Creating NVTabular Dataset...\")\n",
    "    print(\"   Using 32MB partitions for memory efficiency\")\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    dataset = Dataset(\n",
    "        temp_path,\n",
    "        engine='parquet',\n",
    "        part_size='32MB'  #change size based on your environment\n",
    "    )\n",
    "    print(\"   ‚úÖ Dataset created\")\n",
    "    \n",
    "    # Create and fit workflow\n",
    "    print(\"\\nüìä Fitting workflow...\")\n",
    "    workflow = create_workflow()\n",
    "    workflow.fit(dataset)\n",
    "    print(\"   ‚úÖ Workflow fitted\")\n",
    "    \n",
    "    # Transform and save\n",
    "    print(f\"\\nüíæ Transforming and saving to {OUTPUT_DIR}...\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        workflow.transform(dataset).to_parquet(\n",
    "            output_path=OUTPUT_DIR,\n",
    "            shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "            out_files_per_proc=8\n",
    "        )\n",
    "        \n",
    "        workflow_path = f'{OUTPUT_DIR}/workflow'\n",
    "        workflow.save(workflow_path)\n",
    "        print(f\"   ‚úÖ Data processed and saved\")\n",
    "        print(f\"   ‚úÖ Workflow saved to {workflow_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during processing: {e}\")\n",
    "        if os.path.exists(OUTPUT_DIR):\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "        raise\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    final_mem = print_memory()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processing complete!\")\n",
    "    print(f\"   Time: {elapsed:.1f}s\")\n",
    "    print(f\"   Memory increase: +{final_mem - initial_mem:.1f}%\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    return OUTPUT_DIR\n",
    "\n",
    "# Process data\n",
    "processed_dir = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîÑ Stratified KFold Cross-Validation\n",
      "======================================================================\n",
      "\n",
      "üì¶ Loading processed data...\n",
      "   Converting to GPU DataFrame...\n",
      "   ‚úÖ Loaded 10,704,179 rows x 118 columns\n",
      "   Time: 2.3s\n",
      "üíæ CPU: 58.2GB/472.2GB (13.0%)\n",
      "üíæ GPU: 6.4GB/48.0GB\n",
      "\n",
      "üìä Preparing data for XGBoost...\n",
      "   Shape: (10704179, 117)\n",
      "   Features: 117\n",
      "   Samples: 10,704,179\n",
      "\n",
      "üìä Class distribution:\n",
      "   Positive ratio: 0.0191\n",
      "   Scale pos weight: 51.43\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üîÑ Starting cross-validation...\n",
      "\n",
      "üìç Fold 1/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   üìä Results:\n",
      "      Score: 0.353420\n",
      "      AP: 0.081463\n",
      "      WLL: 0.599035\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 26.0s\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üìç Fold 2/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   üìä Results:\n",
      "      Score: 0.354111\n",
      "      AP: 0.082687\n",
      "      WLL: 0.598632\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 26.1s\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üìç Fold 3/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   üìä Results:\n",
      "      Score: 0.352495\n",
      "      AP: 0.080028\n",
      "      WLL: 0.600096\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 26.0s\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üìç Fold 4/5\n",
      "   Train: 8,563,343 | Val: 2,140,836\n",
      "   Training...\n",
      "   üìä Results:\n",
      "      Score: 0.353601\n",
      "      AP: 0.081818\n",
      "      WLL: 0.599018\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 25.8s\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üìç Fold 5/5\n",
      "   Train: 8,563,344 | Val: 2,140,835\n",
      "   Training...\n",
      "   üìä Results:\n",
      "      Score: 0.352288\n",
      "      AP: 0.079672\n",
      "      WLL: 0.600247\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 26.0s\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "======================================================================\n",
      "üìä Final Cross-Validation Results\n",
      "======================================================================\n",
      "\n",
      "üèÜ Competition Score: 0.353183 ¬± 0.000688\n",
      "üìà Average Precision: 0.081133 ¬± 0.001127\n",
      "üìâ Weighted LogLoss: 0.599406 ¬± 0.000644\n",
      "\n",
      "All fold scores: ['0.353420', '0.354111', '0.352495', '0.353601', '0.352288']\n"
     ]
    }
   ],
   "source": [
    "def run_cv(processed_dir, n_folds=5):\n",
    "    \"\"\"Run stratified cross-validation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîÑ Stratified KFold Cross-Validation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load processed data\n",
    "    print(\"\\nüì¶ Loading processed data...\")\n",
    "    start_load = time.time()\n",
    "    \n",
    "    try:\n",
    "        dataset = Dataset(processed_dir, engine='parquet', part_size='256MB')\n",
    "        print(\"   Converting to GPU DataFrame...\")\n",
    "        gdf = dataset.to_ddf().compute()\n",
    "        print(f\"   ‚úÖ Loaded {len(gdf):,} rows x {len(gdf.columns)} columns\")\n",
    "        print(f\"   Time: {time.time() - start_load:.1f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print_memory()\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nüìä Preparing data for XGBoost...\")\n",
    "    y = gdf['clicked'].to_numpy()\n",
    "    X = gdf.drop('clicked', axis=1)\n",
    "    \n",
    "    # Convert to float32\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype != 'float32':\n",
    "            X[col] = X[col].astype('float32')\n",
    "    \n",
    "    X_np = X.to_numpy()\n",
    "    print(f\"   Shape: {X_np.shape}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Samples: {X.shape[0]:,}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    pos_ratio = y.mean()\n",
    "    scale_pos_weight = (1 - pos_ratio) / pos_ratio\n",
    "    print(f\"\\nüìä Class distribution:\")\n",
    "    print(f\"   Positive ratio: {pos_ratio:.4f}\")\n",
    "    print(f\"   Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    del X, gdf\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'gpu_id': 0,\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    print(\"\\nüîÑ Starting cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = []\n",
    "    cv_ap = []\n",
    "    cv_wll = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_np, y), 1):\n",
    "        print(f\"\\nüìç Fold {fold}/{n_folds}\")\n",
    "        fold_start = time.time()\n",
    "        \n",
    "        # Create DMatrix\n",
    "        print(f\"   Train: {len(train_idx):,} | Val: {len(val_idx):,}\")\n",
    "        dtrain = xgb.DMatrix(X_np[train_idx], label=y[train_idx])\n",
    "        dval = xgb.DMatrix(X_np[val_idx], label=y[val_idx])\n",
    "        \n",
    "        # Train\n",
    "        print(\"   Training...\")\n",
    "        model = xgb.train(\n",
    "            params, dtrain,\n",
    "            num_boost_round=200,\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(dval)\n",
    "        score, ap, wll = calculate_competition_score(y[val_idx], y_pred)\n",
    "        \n",
    "        cv_scores.append(score)\n",
    "        cv_ap.append(ap)\n",
    "        cv_wll.append(wll)\n",
    "        \n",
    "        print(f\"   üìä Results:\")\n",
    "        print(f\"      Score: {score:.6f}\")\n",
    "        print(f\"      AP: {ap:.6f}\")\n",
    "        print(f\"      WLL: {wll:.6f}\")\n",
    "        print(f\"      Best iteration: {model.best_iteration}\")\n",
    "        print(f\"   ‚è±Ô∏è Time: {time.time() - fold_start:.1f}s\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del dtrain, dval, model\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    # Final results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä Final Cross-Validation Results\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüèÜ Competition Score: {np.mean(cv_scores):.6f} ¬± {np.std(cv_scores):.6f}\")\n",
    "    print(f\"üìà Average Precision: {np.mean(cv_ap):.6f} ¬± {np.std(cv_ap):.6f}\")\n",
    "    print(f\"üìâ Weighted LogLoss: {np.mean(cv_wll):.6f} ¬± {np.std(cv_wll):.6f}\")\n",
    "    \n",
    "    print(f\"\\nAll fold scores: {[f'{s:.6f}' for s in cv_scores]}\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Run cross-validation\n",
    "cv_scores = run_cv(processed_dir, N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ\n",
      "COMPLETE!\n",
      "üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ\n",
      "\n",
      "‚úÖ Final CV Score: 0.353183 ¬± 0.000688\n",
      "‚úÖ Full dataset processed (10.7M rows)\n",
      "‚úÖ XGBoost-optimized preprocessing (no normalization)\n",
      "‚úÖ Memory-efficient with small partitions\n",
      "======================================================================\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üßπ Final cleanup complete\n",
      "üíæ CPU: 59.1GB/472.2GB (13.2%)\n",
      "üíæ GPU: 1.3GB/48.0GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final summary\n",
    "if cv_scores:\n",
    "    print(\"\\n\" + \"üéâ\"*35)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"üéâ\"*35)\n",
    "    print(f\"\\n‚úÖ Final CV Score: {np.mean(cv_scores):.6f} ¬± {np.std(cv_scores):.6f}\")\n",
    "    print(\"‚úÖ Full dataset processed (10.7M rows)\")\n",
    "    print(\"‚úÖ XGBoost-optimized preprocessing (no normalization)\")\n",
    "    print(\"‚úÖ Memory-efficient with small partitions\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Cross-validation did not complete. Please check for errors above.\")\n",
    "\n",
    "# Final cleanup\n",
    "clear_gpu_memory()\n",
    "print(\"\\nüßπ Final cleanup complete\")\n",
    "print_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
